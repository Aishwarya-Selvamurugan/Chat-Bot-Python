{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aishwarya-Selvamurugan/Chat-Bot-Python/blob/main/Basic_chatbot_using_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCC9Aga6P-8S"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VBjY7aTBCl7"
      },
      "source": [
        "# Importing and reading corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRRSQAciQjNF",
        "outputId": "2498b52a-f0ab-45f0-a64b-f066c5e17b27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "f = open('chatbot.txt','r',errors = 'ignore') \n",
        "raw_doc = f.read()\n",
        "raw_doc = raw_doc.lower()\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "sent_tokens = nltk.sent_tokenize(raw_doc)\n",
        "word_tokens = nltk.word_tokenize(raw_doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHhA-t9tT7hU",
        "outputId": "9193ad49-a4da-4bcc-e7b4-479f5ca21056"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from noisy, structured and unstructured data,[1][2] and apply knowledge from data across a broad range of application domains.',\n",
              " 'data science is related to data mining, machine learning and big data.']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "sent_tokens[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tjHlo-nT_IM",
        "outputId": "af4d1175-4c7a-4ab2-fa04-c6ccf32aea3a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data', 'science']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "word_tokens[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXRUZWJVUQEh"
      },
      "source": [
        "Text preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6_vrdj341cV",
        "outputId": "d8500eaa-190e-4258-8142-0d18c6ab064f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LplV2UMXCzml"
      },
      "source": [
        "Text Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRekQfpsUI-Y"
      },
      "outputs": [],
      "source": [
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "def LemTokens(tokens):\n",
        "  return [lemmer.lemmatize(token) for token in tokens]\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "def LemNormalize(text):\n",
        "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbsA79mZVL2O"
      },
      "source": [
        "Defining the greeting function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNOIcNrcVKAZ"
      },
      "outputs": [],
      "source": [
        "greet_inputs = (\"hello\",\"hi\",\"hey\",\"greetings\")\n",
        "greet_responses = [\"hi\",\"hey\",\"hello\",\"I am glad! You are talking to me\"]\n",
        "def greet(sentence):\n",
        "  for word in sentence.split():\n",
        "    if word.lower() in greet_inputs:\n",
        "      return random.choice(greet_responses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-6h37MiWmOa"
      },
      "source": [
        "\n",
        "Response generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_epRYpSnWGSD"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwuQ7gicXC-w"
      },
      "outputs": [],
      "source": [
        "def response(user_response):\n",
        "  robo1_response=''\n",
        "  TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
        "  tfidf = TfidfVec.fit_transform(sent_tokens)\n",
        "  vals = cosine_similarity(tfidf[-1],tfidf)\n",
        "  idx = vals.argsort()[0][-2]\n",
        "  flat = vals.flatten()\n",
        "  flat.sort()\n",
        "  req_tfidf = flat[-2]\n",
        "  if(req_tfidf == 0):\n",
        "    robo1_response = robo1_response+\"I am sorry! I don't understand you\"\n",
        "    return robo1_response\n",
        "  else:\n",
        "    robo1_response = robo1_response+sent_tokens[idx]\n",
        "    return robo1_response     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAPpVt0baBux"
      },
      "source": [
        "\n",
        "Defining conversation start/end protocol"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7jvhAnKZ1U4",
        "outputId": "fe926404-dbbd-4834-a563-52b758ffdc60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BOT : My name is Jarvis\n",
            "hi\n",
            "BOT : I am glad! You are talking to me\n",
            "hello\n",
            "BOT : hi\n",
            "data science\n",
            "BOT : "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3] however, data science is different from computer science and information science.\n",
            "stanford\n",
            "BOT : [18]\n",
            "\n",
            "stanford professor david donoho writes that data science is not distinguished from statistics by the size of datasets or use of computing, and that many graduate programs misleadingly advertise their analytics and statistics training as the essence of a data-science program.\n",
            "bye\n",
            "BOT : GoodBye! Take care <3\n"
          ]
        }
      ],
      "source": [
        "flag = True\n",
        "print(\"BOT : My name is Jarvis\")\n",
        "while(flag == True):\n",
        "  user_response = input()\n",
        "  user_response = user_response.lower()\n",
        "  if(user_response != 'bye'):\n",
        "    if(user_response == 'thanks' or user_response == 'thank you'):\n",
        "      flag = False\n",
        "      print(\"BOT : You are welome..\")\n",
        "    else:\n",
        "      if(greet(user_response)!=None):\n",
        "        print(\"BOT : \"+greet(user_response))\n",
        "      else:\n",
        "        sent_tokens.append(user_response)\n",
        "        # word_tokens = word_tokens+nltk.word_tokenize(user_response)\n",
        "        # final_words = list(set(word_tokens))\n",
        "        print(\"BOT : \",end=\"\")\n",
        "        print(response(user_response))\n",
        "        sent_tokens.remove(user_response)\n",
        "  else:\n",
        "    flag =  False\n",
        "    print(\"BOT : GoodBye! Take care <3\")         "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pF_60_Ni51-Z"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lp-fHhkncWEQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgYxhmbO7JGFodIeRrqXoh",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}